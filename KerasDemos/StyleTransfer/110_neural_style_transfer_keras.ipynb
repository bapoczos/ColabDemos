{"cells":[{"cell_type":"markdown","metadata":{"id":"tkDX6fy3-FxQ"},"source":["# Neural style transfer\n","\n","**Based on the work of:** [fchollet](https://twitter.com/fchollet)\u003cbr\u003e\n","**Description:** Transfering the style of a reference image to target image using gradient descent."]},{"cell_type":"markdown","metadata":{"id":"WVkd6Avt-FxV"},"source":["## Introduction\n","\n","Style transfer consists in generating an image\n","with the same \"content\" as a base image, but with the\n","\"style\" of a different picture (typically artistic).\n","This is achieved through the optimization of a loss function\n","that has 3 components: \"style loss\", \"content loss\",\n","and \"total variation loss\":\n","\n","- The total variation loss imposes local spatial continuity between\n","the pixels of the combination image, giving it visual coherence.\n","- The style loss is where the deep learning keeps in --that one is defined\n","using a deep convolutional neural network. Precisely, it consists in a sum of\n","L2 distances between the Gram matrices of the representations of\n","the base image and the style reference image, extracted from\n","different layers of a convnet (trained on ImageNet). The general idea\n","is to capture color/texture information at different spatial\n","scales (fairly large scales --defined by the depth of the layer considered).\n","- The content loss is a L2 distance between the features of the base\n","image (extracted from a deep layer) and the features of the combination image,\n","keeping the generated image close enough to the original one.\n","\n","**Reference:** [A Neural Algorithm of Artistic Style](\n","  http://arxiv.org/abs/1508.06576)"]},{"cell_type":"markdown","metadata":{"id":"sASeZUws-FxW"},"source":["## Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2419,"status":"ok","timestamp":1667112842356,"user":{"displayName":"Barnabas Poczos","userId":"09968637475397605684"},"user_tz":240},"id":"2bh2F6CZP7j1"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.applications import vgg19\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1667112842357,"user":{"displayName":"Barnabas Poczos","userId":"09968637475397605684"},"user_tz":240},"id":"0VuKdGJuP--1"},"outputs":[],"source":["base_image_path = keras.utils.get_file(\"content.jpg\", \"https://i.imgur.com/VRAFNC4.jpeg\")\n","\n","style_reference_image_path = keras.utils.get_file(\n","    \"style.jpg\", \"https://i.imgur.com/9ooB60I.jpg\"\n",")\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1667112842357,"user":{"displayName":"Barnabas Poczos","userId":"09968637475397605684"},"user_tz":240},"id":"zTCcVDFF-FxW"},"outputs":[],"source":["\n","result_prefix = \"image_generated\"\n","\n","# Weights of the different loss components\n","total_variation_weight = 1e-6\n","style_weight = 1e-5\n","content_weight = 1.5e-10\n","\n","# Dimensions of the generated picture.\n","width, height = keras.preprocessing.image.load_img(base_image_path).size\n","img_nrows = 800\n","img_ncols = int(width * img_nrows / height)"]},{"cell_type":"markdown","metadata":{"id":"Ox71LffD-FxX"},"source":["## Let's take a look at our base (content) image and our style reference image"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1xJQZyk4BPo79MPKsPJCZSdRaUcq90O5j"},"executionInfo":{"elapsed":29967,"status":"ok","timestamp":1667112872318,"user":{"displayName":"Barnabas Poczos","userId":"09968637475397605684"},"user_tz":240},"id":"A3zJtelK-FxY","outputId":"56bb2bca-c968-48a1-88d0-48ec2b31c77d"},"outputs":[{"data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{},"output_type":"display_data"}],"source":["from IPython.display import Image, display\n","\n","display(Image(base_image_path))\n","display(Image(style_reference_image_path))"]},{"cell_type":"markdown","metadata":{"id":"6N2tHFM0-FxY"},"source":["## Image preprocessing / deprocessing utilities"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1667112872319,"user":{"displayName":"Barnabas Poczos","userId":"09968637475397605684"},"user_tz":240},"id":"q-EQCl0S-FxZ"},"outputs":[],"source":["\n","def preprocess_image(image_path):\n","    # Util function to open, resize and format pictures into appropriate tensors\n","    img = keras.preprocessing.image.load_img(\n","        image_path, target_size=(img_nrows, img_ncols)\n","    )\n","    img = keras.preprocessing.image.img_to_array(img)\n","    img = np.expand_dims(img, axis=0)\n","    img = vgg19.preprocess_input(img)\n","    return tf.convert_to_tensor(img)\n","\n","\n","def deprocess_image(x):\n","    # Util function to convert a tensor into a valid image\n","    x = x.reshape((img_nrows, img_ncols, 3))\n","    # Remove zero-center by mean pixel\n","    x[:, :, 0] += 103.939\n","    x[:, :, 1] += 116.779\n","    x[:, :, 2] += 123.68\n","    # 'BGR'-\u003e'RGB'\n","    x = x[:, :, ::-1]\n","    x = np.clip(x, 0, 255).astype(\"uint8\")\n","    return x\n"]},{"cell_type":"markdown","metadata":{"id":"HLWsDUeN-FxZ"},"source":["## Compute the style transfer loss\n","\n","First, we need to define 4 utility functions:\n","\n","- `gram_matrix` (used to compute the style loss)\n","- The `style_loss` function, which keeps the generated image close to the local textures\n","of the style reference image\n","- The `content_loss` function, which keeps the high-level representation of the\n","generated image close to that of the base image\n","- The `total_variation_loss` function, a regularization loss which keeps the generated\n","image locally-coherent"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1667112872319,"user":{"displayName":"Barnabas Poczos","userId":"09968637475397605684"},"user_tz":240},"id":"9kB-t1rW-Fxa"},"outputs":[],"source":["# The gram matrix of an image tensor (feature-wise outer product)\n","\n","\n","def gram_matrix(x):\n","    x = tf.transpose(x, (2, 0, 1))\n","    features = tf.reshape(x, (tf.shape(x)[0], -1))\n","    gram = tf.matmul(features, tf.transpose(features))\n","    return gram\n","\n","\n","# The \"style loss\" is designed to maintain\n","# the style of the reference image in the generated image.\n","# It is based on the gram matrices (which capture style) of\n","# feature maps from the style reference image\n","# and from the generated image\n","\n","\n","def style_loss(style, combination):\n","    S = gram_matrix(style)\n","    C = gram_matrix(combination)\n","    channels = 3\n","    size = img_nrows * img_ncols\n","    return tf.reduce_sum(tf.square(S - C)) / (4.0 * (channels**2) * (size**2))\n","\n","\n","# An auxiliary loss function\n","# designed to maintain the \"content\" of the\n","# base image in the generated image\n","\n","\n","def content_loss(base, combination):\n","    return tf.reduce_sum(tf.square(combination - base))\n","\n","\n","# The 3rd loss function, total variation loss,\n","# designed to keep the generated image locally coherent\n","\n","\n","def total_variation_loss(x):\n","    a = tf.square(\n","        x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, 1:, : img_ncols - 1, :]\n","    )\n","    b = tf.square(\n","        x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, : img_nrows - 1, 1:, :]\n","    )\n","    return tf.reduce_sum(tf.pow(a + b, 1.25))\n"]},{"cell_type":"markdown","metadata":{"id":"MHQkTYfq-Fxb"},"source":["Next, let's create a feature extraction model that retrieves the intermediate activations\n","of VGG19 (as a dict, by name)."]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":340,"status":"ok","timestamp":1667112872652,"user":{"displayName":"Barnabas Poczos","userId":"09968637475397605684"},"user_tz":240},"id":"1ioOXL6P-Fxb"},"outputs":[],"source":["# Build a VGG19 model loaded with pre-trained ImageNet weights\n","model = vgg19.VGG19(weights=\"imagenet\", include_top=False)\n","\n","# Get the symbolic outputs of each \"key\" layer (we gave them unique names).\n","outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n","\n","# Set up a model that returns the activation values for every layer in\n","# VGG19 (as a dict).\n","feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)"]},{"cell_type":"markdown","metadata":{"id":"79tVpSYi-Fxb"},"source":["Finally, here's the code that computes the style transfer loss."]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1667112872653,"user":{"displayName":"Barnabas Poczos","userId":"09968637475397605684"},"user_tz":240},"id":"jEbU4RZW-Fxc"},"outputs":[],"source":["# List of layers to use for the style loss.\n","style_layer_names = [\n","    \"block1_conv1\",\n","    \"block2_conv1\",\n","    \"block3_conv1\",\n","    \"block4_conv1\",\n","    \"block5_conv1\",\n","]\n","# The layer to use for the content loss.\n","content_layer_name = \"block5_conv2\"\n","\n","\n","def compute_loss(combination_image, base_image, style_reference_image):\n","    input_tensor = tf.concat(\n","        [base_image, style_reference_image, combination_image], axis=0\n","    )\n","    features = feature_extractor(input_tensor)\n","\n","    # Initialize the loss\n","    loss = tf.zeros(shape=())\n","\n","    # Add content loss\n","    layer_features = features[content_layer_name]\n","    base_image_features = layer_features[0, :, :, :]\n","    combination_features = layer_features[2, :, :, :]\n","    loss = loss + content_weight * content_loss(\n","        base_image_features, combination_features\n","    )\n","    # Add style loss\n","    for layer_name in style_layer_names:\n","        layer_features = features[layer_name]\n","        style_reference_features = layer_features[1, :, :, :]\n","        combination_features = layer_features[2, :, :, :]\n","        sl = style_loss(style_reference_features, combination_features)\n","        loss += (style_weight / len(style_layer_names)) * sl\n","\n","    # Add total variation loss\n","    loss += total_variation_weight * total_variation_loss(combination_image)\n","    return loss\n"]},{"cell_type":"markdown","metadata":{"id":"rLltme7c-Fxc"},"source":["## Add a tf.function decorator to loss \u0026 gradient computation\n","\n","To compile it, and thus make it fast."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1667112872654,"user":{"displayName":"Barnabas Poczos","userId":"09968637475397605684"},"user_tz":240},"id":"4UT9Zxx1-Fxc"},"outputs":[],"source":["\n","@tf.function\n","def compute_loss_and_grads(combination_image, base_image, style_reference_image):\n","    with tf.GradientTape() as tape:\n","        loss = compute_loss(combination_image, base_image, style_reference_image)\n","    grads = tape.gradient(loss, combination_image)\n","    return loss, grads\n"]},{"cell_type":"markdown","metadata":{"id":"BttYn_Mz-Fxc"},"source":["## The training loop\n","\n","Repeatedly run vanilla gradient descent steps to minimize the loss, and save the\n","resulting image every 100 iterations.\n","\n","We decay the learning rate by 0.96 every 100 steps."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":359128,"status":"ok","timestamp":1667113231766,"user":{"displayName":"Barnabas Poczos","userId":"09968637475397605684"},"user_tz":240},"id":"NTVC2G3u-Fxd","outputId":"b858636c-2823-4352-ac2a-b427d49fde4c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration 100: loss=34989.79\n","Iteration 200: loss=29391.82\n","Iteration 300: loss=26444.89\n","Iteration 400: loss=24902.30\n","Iteration 500: loss=24029.15\n","Iteration 600: loss=23429.01\n","Iteration 700: loss=22880.95\n","Iteration 800: loss=22469.94\n","Iteration 900: loss=22148.76\n","Iteration 1000: loss=21879.63\n","Iteration 1100: loss=21651.09\n","Iteration 1200: loss=21455.04\n","Iteration 1300: loss=21285.68\n","Iteration 1400: loss=21138.61\n","Iteration 1500: loss=21009.82\n","Iteration 1600: loss=20895.78\n","Iteration 1700: loss=20793.68\n","Iteration 1800: loss=20701.67\n","Iteration 1900: loss=20618.30\n","Iteration 2000: loss=20542.63\n","Iteration 2100: loss=20473.47\n","Iteration 2200: loss=20409.99\n","Iteration 2300: loss=20351.56\n","Iteration 2400: loss=20297.58\n","Iteration 2500: loss=20247.69\n","Iteration 2600: loss=20201.43\n","Iteration 2700: loss=20158.43\n","Iteration 2800: loss=20118.36\n","Iteration 2900: loss=20080.97\n","Iteration 3000: loss=20046.01\n","Iteration 3100: loss=20013.30\n","Iteration 3200: loss=19982.54\n","Iteration 3300: loss=19953.72\n","Iteration 3400: loss=19926.61\n","Iteration 3500: loss=19901.00\n","Iteration 3600: loss=19876.86\n","Iteration 3700: loss=19854.13\n","Iteration 3800: loss=19832.59\n","Iteration 3900: loss=19812.26\n","Iteration 4000: loss=19793.02\n"]}],"source":["optimizer = keras.optimizers.SGD(\n","    keras.optimizers.schedules.ExponentialDecay(\n","        initial_learning_rate=100.0, decay_steps=100, decay_rate=0.96\n","    )\n",")\n","\n","base_image = preprocess_image(base_image_path)\n","style_reference_image = preprocess_image(style_reference_image_path)\n","combination_image = tf.Variable(preprocess_image(base_image_path))\n","\n","iterations = 4000\n","for i in range(1, iterations + 1):\n","    loss, grads = compute_loss_and_grads(\n","        combination_image, base_image, style_reference_image\n","    )\n","    optimizer.apply_gradients([(grads, combination_image)])\n","    if i % 100 == 0:\n","        print(\"Iteration %d: loss=%.2f\" % (i, loss))\n","        img = deprocess_image(combination_image.numpy())\n","        fname = result_prefix + \"_at_iteration_%d.png\" % i\n","        keras.preprocessing.image.save_img(fname, img)"]},{"cell_type":"markdown","metadata":{"id":"y6dP92jN-Fxd"},"source":["After 4000 iterations, you get the following result:"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":519,"output_embedded_package_id":"1Ta_pvKjhgnhFWuOWLxjpCiIaSLE2vaax"},"executionInfo":{"elapsed":51,"status":"ok","timestamp":1667113231767,"user":{"displayName":"Barnabas Poczos","userId":"09968637475397605684"},"user_tz":240},"id":"_EWHWbPA-Fxd","outputId":"dc5ce0e7-a80e-4e85-d4a4-754895aeb733"},"outputs":[{"data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{},"output_type":"display_data"}],"source":["display(Image(result_prefix + \"_at_iteration_4000.png\"))"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":519,"output_embedded_package_id":"1-OZWvKA3oMemryBiXGXhGA_LuuDjPwxy"},"executionInfo":{"elapsed":23851,"status":"ok","timestamp":1667113255611,"user":{"displayName":"Barnabas Poczos","userId":"09968637475397605684"},"user_tz":240},"id":"5xAx3NQ1Wp8E","outputId":"fa7e7290-e6f5-46da-935a-8162f95902d3"},"outputs":[],"source":["display(Image(base_image_path))"]},{"cell_type":"markdown","metadata":{"id":"MjEFjDBHqvJh"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BQBUdKO1qvc0"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"","provenance":[{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/generative/ipynb/neural_style_transfer.ipynb","timestamp":1666934693254}],"toc_visible":true,"version":""},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":0}